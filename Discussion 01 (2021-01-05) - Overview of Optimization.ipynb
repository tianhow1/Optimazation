{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 1: Overview of Optimization\n",
    "\n",
    "In this discussion, we will talk about\n",
    "* The general task of optimization\n",
    "* The different types of optimization problems and algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire field of **optimization** concerns one broad topic only:\n",
    "\n",
    ">Given some (usually scalar) function $f(\\mathbf{x})$ of in general many variables (i.e. $\\mathbf{x}\\in\\mathbb{R}^n$, $n\\ge1$), find a point $\\mathbf{x}^*$ such that $f(\\mathbf{x}^*)\\le f(\\mathbf{x})$ for all other $\\mathbf{x}$, i.e. $\\mathbf{x}^*$ is a minimizer of $f$.\n",
    "\n",
    "This may seem like a very niche and abstract topic, but a large number of problems in many different fields can be essentially boiled down to this idea. For several interesting examples of optimization problems in various fields, see [here](https://neos-guide.org/Case-Studies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, finding a minimum of a function, even one with many inputs, is straightforward. A **sufficient condition** for (at least a local) minimum is that\n",
    "1. $\\nabla f(\\mathbf{x}^*)=\\mathbf{0}$.\n",
    "2. $\\nabla^2 f(\\mathbf{x}^*)$ is a [symmetric positive definite (SPD)](https://en.wikipedia.org/wiki/Definite_symmetric_matrix) matrix.\n",
    "\n",
    "In practice, however, finding such an $\\mathbf{x}^*$ can be extrememly difficult, particularly if the system of equations is highly nonlinear. This entire class is devoted to \n",
    "1. Describing various types of optimization (constrained vs. unconstrained, continuous vs. discrete, local vs. global linear vs. nonlinear, etc.). See [here](https://neos-guide.org/content/optimization-taxonomy) for an excellent graphic \"taxonomy\" of optimization types.\n",
    "2. Outlining various algorithms for solving optimization problems of some of the types. See [here](https://neos-guide.org/content/algorithms-by-type) for a (nonexhaustive) list of algorithms.\n",
    "\n",
    "We will mainly focus on *continuous, unconstrained* optimization, though we will discuss a few continuous, constrained optimization problems such as [linear programming](https://en.wikipedia.org/wiki/Linear_programming), which has many applications, e.g. in business and economics. We will not discuss discrete optimization, which is in general a much more difficult problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example problem: [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function)\n",
    "\n",
    "Most problems (e.g. on homework) in this class will require programming, but here is one we can do by hand:\n",
    "\n",
    "**Problem:** Compute the gradient $\\nabla f(x)$ and Hessian $\\nabla^2 f(x)$ of the Rosenbrock function\n",
    "\n",
    "$$f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1-x_1)^2$$\n",
    "\n",
    "and show that $\\mathbf{x}^*=(1, 1)$ is the only local minimizer of this function, and that the Hessian matrix at that point is positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** Since $\\nabla f(\\mathbf{x})=\\left\\langle \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}\\right\\rangle$, and\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\frac{\\partial f}{\\partial x_1} & = -400x_1(x_2-x_1^2)-2(1-x_1)\\\\\n",
    "    \\frac{\\partial f}{\\partial x_2} & = 200(x_2-x_1^2)\n",
    "\\end{align*} $$\n",
    "\n",
    "to satisfy $\\nabla f=\\mathbf{0}$, we must have from the second component, $200(x_2-x_1^2)=0\\implies x_2=x_1^2$. Plugging this into the equation for the second component, the first term vanishes and we have $-2(1-x_1)=0\\implies x_1=1$, so $x_2=1^2=1$. Thus at $\\mathbf{x}^*=(1,1)$ and *only* at this $\\mathbf{x}^*$, $\\nabla f(\\mathbf{x}^*)=\\mathbf{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hessian, we have\n",
    "$$ \\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "    \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2} \\\\\n",
    "    \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    -400x_1\\cdot-2x_1-400(x_2-x_1^2)+2 & -400x_1 \\\\\n",
    "    -400x_1 & 200\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    800x_1^2-400(x_2-x_1^2)+2 & -400x_1 \\\\\n",
    "    -400x_1 & 200\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "so at $\\mathbf{x}^*=(1,1)$, we have\n",
    "\n",
    "$$ \\nabla^2 f(1,1) = \\begin{bmatrix}\n",
    "    802 & -400 \\\\\n",
    "    -400 & 200\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "The eigenvalues of this matrix are solutions to the characteristic equation\n",
    "\n",
    "$$ (802-\\lambda)(200-\\lambda)-160000=0 \\implies \\lambda^2 - 1002\\lambda + 400 = 0 \\implies \\lambda=501\\pm\\sqrt{250601},$$\n",
    "\n",
    "both of which are (only barely!) positive, so the Hessian matrix is positive definite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
