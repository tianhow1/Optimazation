{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton Methods\n",
    "Quasi-Newton methods approximate the Hessian with a positive definite $B_k$ during each iteration, or they approximate the inverse of the Hessian with a matrix $H_k$.\n",
    "\n",
    "An iteration of Quasi-Newton takes the form\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_kB_k^{-1}\\nabla f_k$$\n",
    "Or\n",
    "$$\\mathbf{x}_{k+1} =  \\mathbf{x}_k - \\alpha_kH_k\\nabla f_k$$\n",
    "\n",
    "where $B_k$ is a positive definite approximation of the Hessian $\\nabla^2 f_k$ and/or $H_k$ is a positive definite approximation of the inverse Hessian $[\\nabla^2f_k]^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we choose $B_k$ such that it has the same property, that is determine a solution to\n",
    "\n",
    "$$ B_{k+1}(\\mathbf{x}_{k+1}-\\mathbf{x}_k)= \\nabla f_{k+1}-\\nabla f_k,\\qquad\\text{or}\\qquad B_{k+1}\\mathbf{s}_k = \\mathbf{y}_k $$\n",
    "\n",
    "with $\\mathbf{s}_k\\equiv \\mathbf{x}_{k+1}-\\mathbf{x}_k$ and $\\mathbf{y}_k\\equiv\\nabla f_{k+1}-\\nabla f_k$. The above, called the **secant equation**, is equivalent to the following requirements for the inverse approximation, $H_{k+1}$, sometimes called the **dual secant equation**:\n",
    "\n",
    "$$ H_{k+1}(\\nabla f_{k+1}-\\nabla f_k)=\\mathbf{x}_{k+1}-\\mathbf{x}_k,\\qquad\\text{or}\\qquad H_{k+1}\\mathbf{y}_k =\\mathbf{s}_k $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davidon-Fletcher-Powell (DFP) method\n",
    "\n",
    "* Idea: Minimize $B_{k+1}$ such that\n",
    "$$ B_{k+1} = \\min_B \\|B-B_k\\|_{F,W}\\qquad \\text{subject to } B=B^T, B\\mathbf{s}_k=\\mathbf{y}_k $$\n",
    "\n",
    "Results in\n",
    "$$ B_{k+1} = \\left(I-\\frac{\\mathbf{y}_k\\mathbf{s}_k^T}{\\mathbf{y}_k^T\\mathbf{s}_k}\\right)B_k\\left(I-\\frac{\\mathbf{s}_k\\mathbf{y}_k^T}{\\mathbf{y}_k^T\\mathbf{s}_k}\\right) + \\frac{\\mathbf{y}_k\\mathbf{y}_k^T}{\\mathbf{y}_k^T\\mathbf{s}_k} $$\n",
    "\n",
    "But through the use of **Sherman-Morrison Formula**, we work with the inverse Hessian instead:\n",
    "\n",
    "$$ H_{k+1} = B_{k+1}^{-1} = H_k + \\frac{\\mathbf{s}_k \\mathbf{s}_k^T}{\\mathbf{y}_k^{T} \\mathbf{s}_k} - \\frac{H_k \\mathbf{y}_k \\mathbf{y}_k^T H_k}{\\mathbf{y}_k^T H_k \\mathbf{y}_k} $$\n",
    "\n",
    "Here, $y_k = \\nabla f_{k+1} - \\nabla f_k$, $s_k = x_{k+1} - x_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DFP can \"get stuck\"**: The inverse Hessian can become nearly singular (eigenvalue close to zero) - this will mean each update will not progress very far, even with optimal step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broyden-Fletcher-Goldfarb–Shanno (BFGS) method\n",
    "\n",
    "* Idea: Minimize $H_{k+1}$ such that\n",
    "$$ H_{k+1} = \\min_H \\|H-H_k\\|_{F,W}\\qquad \\text{subject to } H=H^T, H\\mathbf{y}_k=\\mathbf{s}_k $$\n",
    "\n",
    "\n",
    "The algorithm is\n",
    "\n",
    "$$H_{k+1} = H_k + (\\mathbf{y}_k^T\\mathbf{s}_k+\\mathbf{y}_k^T H_k \\mathbf{y}_k)\\frac{\\mathbf{s}_k \\mathbf{s}_k^T}{(\\mathbf{y}_k^T \\mathbf{s}_k)^2} - \\frac{H_k \\mathbf{y}_k \\mathbf{s}_k^T + \\mathbf{s}_k \\mathbf{y}_k^TH_k}{\\mathbf{y}_k^T\\mathbf{s}_k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence**: If $f$ is $\\mathcal{C}^2$ (1st and 2nd derivatives are continuous) and BFGS sequence $x_k$ converges to a minimizer $x$\n",
    "where the Hessian $D^2 f$ is Lipschitz continuous, and if \n",
    "$$\\sum_{k\\ge 1} \\|x_k - x^*\\|<\\infty$$\n",
    "then $x_k \\rightarrow x^*$ at a **superlinear rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For both DFP and BFGS, main benefits are: $H_k$ preserves symmetry and positive definiteness, unlike SR-1.\n",
    "* BFGS has more concrete convergence results than other Quasi-Newton methods, and inverse Hessian tends to avoid the issue of becoming nearly singular.\n",
    "* On quadratic functions, all Quasi-Newton methods will take $n$ steps to converge (where $n$ is the dimension of the vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient Method on Quadratic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version only works on quadratic functions of the form $f(x) = 1/2 x^T Q x - b^T x$.\n",
    "\n",
    "1. Set $\\mathbf{d}_0 = -\\nabla f_0$, as in steepest descent.\n",
    "2. Determine $\\alpha_k$ and update $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k\\mathbf{d}_k$.\n",
    "3. Set $\\mathbf{d}_{k+1} = -\\nabla f_{k+1} + \\beta_k\\mathbf{d}_k$, where $\\beta_k$ is chosen so that $\\mathbf{d}_{k+1}$ and $\\mathbf{d}_k$ are conjugate in some sense.\n",
    "4. Repeat 2-3 until convergence.\n",
    "\n",
    "If $f$ is quadratic, we showed the optimal values for the parameters $\\alpha_k$ and $\\beta_k$ were given by\n",
    "\n",
    "$$ \\alpha_k = -\\frac{\\mathbf{d}_k^T\\nabla f_k}{\\mathbf{d}_k^TQ\\mathbf{d}_k},\\qquad \\beta_k = \\frac{\\mathbf{d}_k^TQ\\nabla f_{k+1}}{\\mathbf{d}_k^TQ\\mathbf{d}_k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WolfeI(alpha,f,x,dx,p,c1=0.1):\n",
    "    '''Return True/False if Wolfe condition I is satisfied for the given alpha'''\n",
    "    LHS = f(x[0]+alpha*p[0], x[1]+alpha*p[1])\n",
    "    RHS = f(x[0],x[1])+c1*alpha*np.dot(dx,p)\n",
    "    return LHS <= RHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG_alg(x0, Q, b, tol=1e-7, max_steps=10000, rho=0.75):\n",
    "    f = lambda x: 1/2 * x @ Q @ x - b @ x\n",
    "    Df = lambda x: Q @ x - b\n",
    "    x=x0\n",
    "    path_CG = [x]\n",
    "    tol = 1e-7            # stop when gradient is smaller than this amount\n",
    "    max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "    rho = 0.75            # rho for backtracking\n",
    "    i=0                   # iteration count\n",
    "    gk = Df(x)          # current gradient\n",
    "    dk = -gk\n",
    "    dks = [dk]\n",
    "    gks = [gk]\n",
    "\n",
    "    while np.linalg.norm(gk)>tol and i<max_steps:    \n",
    "        alpha = - dk @ gk / (dk @ Q @ dk)\n",
    "        xnew = x + alpha*dk\n",
    "        gk1 = Df(xnew)\n",
    "        beta = (dk @ Q @ gk1)/ (dk @ Q @ dk)\n",
    "        dk = -gk1+ beta*dk\n",
    "        dks.append(dk)\n",
    "        path_CG.append(xnew)\n",
    "        x = xnew\n",
    "        gk = gk1\n",
    "        gks.append(gk)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    path_CG=np.array(path_CG)\n",
    "    dks = np.array(dks)\n",
    "    gks = np.array(gks)\n",
    "    print(f'After {i} iterations of CG, approximate minimum is {f(x)} at {x}')\n",
    "    return path_CG, dks, gks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFP(x0, Q, b, tol=1e-7, max_steps=10000, rho=0.75, exact=True):\n",
    "    def f(x, y):\n",
    "        x1 = np.array([x, y])\n",
    "        return 1/2 * x1 @ Q @ x1 - b @ x1\n",
    "    def Df(x, y):\n",
    "        x1 = np.array([x, y])\n",
    "        return Q @ x1 - b\n",
    "    x,y = x0[0], x0[1]    # initial point\n",
    "    path_DFP = [[x,y]]\n",
    "    i=0                   # iteration count\n",
    "    rho = 0.75            # rho for backtracking\n",
    "    H = np.eye(2)         # initial inverse Hessian is identity\n",
    "    dx = Df(x,y)          # current gradient\n",
    "    while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "        pk = -H@dx\n",
    "        if exact:\n",
    "            def subproblem1D(alpha):\n",
    "                return f(x + alpha * pk[0], y + alpha*pk[1])\n",
    "            res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "            print(\"Exact line search step size: {:.4f}\".format(res.x))\n",
    "            alpha = res.x\n",
    "        else:\n",
    "            # backtracking\n",
    "            alpha = 1\n",
    "            while not WolfeI(alpha,f,np.array([x,y]),dx,pk) and alpha>1e-5:  # lower limit to prevent small steps, similar to Wolfe II\n",
    "                alpha *= rho\n",
    "        xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "        path_DFP.append([xnew,ynew])\n",
    "\n",
    "        # secant variables\n",
    "        sk = alpha*pk         # x_{k+1}-x_k\n",
    "        yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k\n",
    "\n",
    "        # DFP update\n",
    "        vec = H@yk # @ is matrix multiplication\n",
    "        denom1 = yk@sk\n",
    "        denom2 = yk@vec\n",
    "        H += np.outer(sk,sk)/denom1 - np.outer(vec,vec)/denom2 # np.outer(vec, vec) = vec * vec^T\n",
    "\n",
    "        x,y = xnew,ynew\n",
    "        dx = Df(x,y)\n",
    "        i += 1\n",
    "    path_DFP=np.array(path_DFP)\n",
    "    print(f'After {i} iterations of DFP, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1e+03 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def BFGS(x0, Q, b, tol=1e-7, max_steps=10000, rho=0.75, exact=True):\n",
    "    def f(x, y):\n",
    "        x1 = np.array([x, y])\n",
    "        return 1/2 * x1 @ Q @ x1 - b @ x1\n",
    "    def Df(x, y):\n",
    "        x1 = np.array([x, y])\n",
    "        return Q @ x1 - b\n",
    "    x,y = x0[0], x0[1]    # initial point\n",
    "    path_BFGS = [[x,y]]\n",
    "    i=0                   # iteration count\n",
    "    rho = 0.75            # rho for backtracking\n",
    "    H = np.eye(2)         # initial inverse Hessian is identity\n",
    "    dx = Df(x,y)          # current gradient\n",
    "    while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "        pk = -H@dx\n",
    "        if exact:\n",
    "            def subproblem1D(alpha):\n",
    "                return f(x + alpha * pk[0], y + alpha*pk[1])\n",
    "            res = minimize_scalar(subproblem1D) # scipy function to minimize objFunction w.r.t alpha\n",
    "            print(\"Exact line search step size: {:.4f}\".format(res.x))\n",
    "            alpha = res.x\n",
    "        else:\n",
    "            # backtracking\n",
    "            alpha = 1\n",
    "            while not WolfeI(alpha,f,np.array([x,y]),dx,pk) and alpha>1e-5:  # lower limit to prevent small steps, similar to Wolfe II\n",
    "                alpha *= rho\n",
    "        xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "        path_BFGS.append([xnew,ynew])\n",
    "\n",
    "        # secant variables\n",
    "        sk = alpha*pk         # x_{k+1}-x_k\n",
    "        yk = Df(xnew,ynew)-dx # Df_{k+1}-Df_k\n",
    "\n",
    "        # BFGS update\n",
    "        vec = H@yk\n",
    "        denom = yk@sk\n",
    "        H += (denom+vec@yk)*np.outer(sk,sk)/denom**2 - (np.outer(vec,sk)+np.outer(sk,vec))/denom\n",
    "\n",
    "        x,y = xnew,ynew\n",
    "        dx = Df(x,y)\n",
    "        i += 1\n",
    "\n",
    "    path_BFGS=np.array(path_BFGS)\n",
    "    print(f'After {i} iterations of BFGS, approximate minimum is {f(x,y)} at {x,y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to demonstrate convergence properties on Quadratic Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact line search step size: 0.1182\n",
      "Exact line search step size: 0.2429\n",
      "After 2 iterations of DFP, approximate minimum is -2.2222222222222228 at (0.22222222222222213, 1.0000000000000002)\n",
      "Exact line search step size: 0.1182\n",
      "Exact line search step size: 0.2350\n",
      "After 2 iterations of BFGS, approximate minimum is -2.222222222222222 at (0.22222222343190004, 0.9999999921748961)\n",
      "After 2 iterations of CG, approximate minimum is -2.2222222222222228 at [0.22222222 1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.5       , 2.        ],\n",
       "        [0.14072155, 1.52720749],\n",
       "        [0.22222222, 1.        ]]), array([[-1.15000000e+01, -4.00000000e+00],\n",
       "        [ 3.46796177e-01, -2.24333777e+00],\n",
       "        [ 6.32143331e-16,  2.19875941e-16]]), array([[ 1.15000000e+01,  4.00000000e+00],\n",
       "        [-7.33506079e-01,  2.10882998e+00],\n",
       "        [-6.66133815e-16,  0.00000000e+00]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[9,0],[0,4]])\n",
    "b = np.array([2,4])\n",
    "x0 = (1.5,2)\n",
    "DFP(x0,Q,b,exact=True)\n",
    "BFGS(x0,Q,b,exact=True)\n",
    "CG_alg(x0,Q,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient CG Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG_alg_efficient(x0, Q, b, tol=1e-7, max_steps=10000, rho=0.75):\n",
    "    f = lambda x: 1/2 * x @ Q @ x - b @ x\n",
    "    Df = lambda x: Q @ x - b\n",
    "    x=x0\n",
    "    path_CG = [x]\n",
    "    tol = 1e-7            # stop when gradient is smaller than this amount\n",
    "    max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "    rho = 0.75            # rho for backtracking\n",
    "    i=0                   # iteration count\n",
    "    \n",
    "    rk = Q @ x0 - b\n",
    "    dk = -rk\n",
    "    dks = [dk]\n",
    "\n",
    "    while np.linalg.norm(rk)>tol and i<max_steps:    \n",
    "        yk = Q @ dk\n",
    "        alpha = rk @ rk / (dk @ yk)\n",
    "        rk1 = rk + alpha * yk # Step 4\n",
    "        xnew = x + alpha*dk\n",
    "        beta = (rk1 @ rk1)/ (rk @ rk)\n",
    "        dk = -rk1+ beta*dk\n",
    "        dks.append(dk)\n",
    "        path_CG.append(xnew)\n",
    "        x = xnew        \n",
    "        rk = rk1\n",
    "        i += 1\n",
    "\n",
    "    path_CG=np.array(path_CG)\n",
    "    dks = np.array(dks)\n",
    "    print(f'After {i} iterations, approximate minimum of efficient CG is {f(x)} at {x}')\n",
    "    return path_CG, dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the time for efficient CG vs regular CG implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 11 iterations, approximate minimum of efficient CG is -2.130254861020093 at [ -4.96960792  -4.3496964   -7.20044619   9.39735763   2.40299444\n",
      "   5.11442835 -13.27780009   2.22084286   3.99167697   5.69108672]\n",
      "Efficient CG takes 0.00100s\n",
      "After 11 iterations of CG, approximate minimum is -2.1302548610200236 at [ -4.96960792  -4.3496964   -7.20044619   9.39735763   2.40299444\n",
      "   5.11442835 -13.27780009   2.22084286   3.99167697   5.69108672]\n",
      "CG takes 0.00000s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "matrixSize = 10\n",
    "Q = np.random.rand(matrixSize, matrixSize)\n",
    "Q = np.dot(Q, Q.transpose())\n",
    "b = np.random.rand(matrixSize)\n",
    "x0 = np.random.rand(matrixSize)\n",
    "start_time = time.time()\n",
    "CG_alg_efficient(x0,Q,b)\n",
    "end_time = time.time() - start_time\n",
    "print(\"Efficient CG takes {:.5f}s\".format(end_time))\n",
    "start_time = time.time()\n",
    "CG_alg(x0,Q,b)\n",
    "end_time = time.time() - start_time\n",
    "print(\"CG takes {:.5f}s\".format(end_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
